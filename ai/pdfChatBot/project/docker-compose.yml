services:
  localai:
    # localai is a self-hosted LLM server that supports various models, including 
    # those from Hugging Face and OpenAI.
    # The image used here includes ffmpeg, which is required for audio processing in the example.
    # For production use, consider using a more minimal image and pre-caching the required models 
    # to reduce startup time.
    # we are going with the core image here, which includes ffmpeg, to ensure that the audio 
    # processing works out of the box.
    # and doesnt include gpu support, which is not required for this example and would increase
    # the image size significantly.
    image: localai/localai:v2.25.0-ffmpeg-core
    ports:
      - 8080:8080
    environment:
      - LOG_LEVEL=INFO
      - MODELS_PATH=/models
    volumes:
      - ../models:/models:cached
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/models"]
      interval: 600s
      timeout: 5s
      retries: 5
      # Allows for initial model downloads
      # In a production environment, you likely want to pre-cache the AI model
      start_period: 100000s
      start_interval: 5s

  client:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - LLM_API_BASE=http://localai:8080/v1
    depends_on:
      localai:
        condition: service_healthy
    restart: "no"
