name: phi-3.5-mini-instruct

# The "parameters" section contains specific settings for the model inference.
# These settings can be adjusted based on your hardware capabilities and performance requirements.
parameters:
  model: bartowski/phi-3.5-mini-instruct.gguf
  type: phi-3.5-mini-instruct

  # In this case, it's set to "gguf", which is a format designed for 
  # efficient inference with the llama-cpp backend.
  format: gguf

# The "backend" field specifies the inference engine to use. 
# In this case, it's set to "llama-cpp", which is a popular choice 
# for running GGUF models efficiently on various hardware configurations.
backend: llama-cpp

# The "context_size" parameter defines the maximum number of tokens that 
# the model can process in a single input.
context_size: 8192

# The "gpu_layers" parameter specifies how many layers of the model should be
# offloaded to the GPU for faster inference. Setting this to 0 means that
# all layers will run on the CPU. 
# You can increase this value if you have a compatible GPU and want to improve performance.
gpu_layers: 0

# Adjust based on your CPU cores. Assume 1 thread per core
threads: 4 

# it's about how many tokens the model will process in one batch during inference.
# You might want to experiment with values 32, 64 or 128
# for a CPU bound 64 is a good starting point, but you can adjust it based on your 
# hardware capabilities and performance requirements.
batch_size: 64 

# The "f16" parameter indicates whether to use 16-bit floating point precision for inference.
# Setting this to true can reduce memory usage and improve performance on compatible hardware,
# but it may also lead to a slight decrease in model accuracy.
f16: true

download_files:
  - filename: bartowski/phi-3.5-mini-instruct.gguf
    uri: https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q6_K.gguf